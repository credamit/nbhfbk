{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install refers to install.py in current directory\n",
    "install_requirements is single function in install.py\n",
    "https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb\n",
    "https://colab.research.google.com/github/credamit/nbhfbk/blob/main/01_introduction.ipynb\n",
    "https://kaggle.com/kernels/welcome?src=https://github.com/credamit/nbhfbk/blob/main/src.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install.py, install_requirements()\n",
    "## requirements.txt [TensorFlow, PyTorch, Scikit-learn, matplotlib etc], Git LFS, transformers, datasets\n",
    "Single function install_requirements from install.py does the following tasks:\n",
    "1. Uses requirements.txt for all chapters except chapter #7, using below command:\n",
    "    python -m pip install -r requirements.txt\n",
    "2. Installs Git LFS, an open source Git extension used to manage large files and binary files in a separate ”LFS store.” It uses below command:\n",
    "    apt install git-lfs\n",
    "3. For chapter 2, it additionally installs Hugging face libraries transformers and datasets using below command:\n",
    "    python -m pip install transformers==4.13.0 datasets==2.8.0\n",
    "\n",
    "Contents of requirements.txt common and chapter 2 specific parts are as below:\n",
    "    transformers[tf,torch,sentencepiece,vision,optuna,sklearn,onnxruntime]==4.16.2\n",
    "    datasets[audio]==1.16.1\n",
    "    matplotlib\n",
    "    ipywidgets\n",
    "    umap-learn==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're on Colab or Kaggle\n",
    "# !git clone https://github.com/nlp-with-transformers/notebooks.git\n",
    "# %cd notebooks\n",
    "# from install import *\n",
    "# install_requirements(is_chapter2=True)\n",
    "!git clone https://github.com/credamit/nbhfbk.git\n",
    "%cd nbhfbk\n",
    "from install import *\n",
    "install_requirements(is_chapter2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils.py, setup_chapter()\n",
    "Here utils refers to utils.py in current directory\n",
    "setup_chapter() is the main functions in util.py, It does the following:\n",
    "1. Checks if GPU is available in current environment or by using below API call for PyTorch\n",
    "    torch.cuda.is_available()\n",
    "2. Displays the versions being used for huggingface libraries transformers and datasets being used in current program.\n",
    "3. Sets Loggging levels to use for both libraries from huggigface\n",
    "4. Sets plot styles to be used when using matplotlib library in our program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "setup_chapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotions (DatasetDict object): Creation\n",
    "1. Library used: datasets\n",
    "2. Function used load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotions (DatasetDict object): Review\n",
    "emotions object is of type DataSetDict class from huggingface library. Lets see its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer (AutoTokenizer object)\n",
    "Retreive tokenizer associated with transformer model for \"distilbert-base-uncased\" By using:\n",
    "1. Class AutoTokenizer in huggingface transformers library\n",
    "2. Method from_pretrained() of Class AutoTokenizer listed above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next three cels, let's review various charactertics of tokenizer retreived from transformer model for \"distilbert-base-uncased\".\n",
    "Here we make use of following attributes from AutoTokenizer class\n",
    "    1. vocab_size\n",
    "    2. model_max_length\n",
    "    3. model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize() function\n",
    "A custom function that uses tokenizer retreived from transformer model for \"distilbert-base-uncased\"\n",
    "    1. This function will later be used as processing function when invoking DatasetDic function map() on emotions DatasetDict object\n",
    "    2. Input is a Dataset object that has a column named \"text\"\n",
    "    2. This function expects a column named \"text\" in input DatasetDict object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotions_encoded (DatasetDict object)\n",
    "1. First argument of map() function here is a processing function that returns additional column names and values that would be appended to emotions dataset. \n",
    "2. We name enhanced emotions dataset as emotions_encoded.\n",
    "3. Two new columns are added \"attention_mask\" and \"input_ids\"\n",
    "4. emotions DatasetDict had two columns, \"text\" and \"label\". \n",
    "5. emotions_encoded has four columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify columns in all three Datasets present in DatasetDict object emotions_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(emotions_encoded[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(emotions_encoded[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model (AutoModel object)\n",
    "Retreiving the model object associated with transformer model named \"distilbert-base-uncased\" by using:\n",
    "    1. Class AutoModel from transformers library from huggingface\n",
    "    2. Method from_pretrained() in class AutoModel mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom function that would be used last_hidden_state using model and tokenizer objects associated with transformer model named \"distilbert-base-uncased\"\n",
    "1. It will later be used as processing function when we invoke map() on DatasetDict object emotions_encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emotions_hidden (DatasetDict object)\n",
    "Using generic function extract_hidden_state created above to derrive last_hidden_state associated with emotions dataset.\n",
    "    1. The input provided is tokenized value of enhanced emotions dataset using the same tokenizer that is also used by the generic function invoked.\n",
    "    2. The output is same input dataset returned back after adding one additional column named hidden_state\n",
    "    3. Value of each hidden_state object contains a numpy object residing in cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating numpy arrays from modified emotions DatasetDict that has additional hidden_state column associated now.\n",
    "We create arrays for both the training data as well as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-1 of 2 to visualize modified training dataset having additional hidden_state column associated with it. [Against same six categories of emotions]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-2 of 2 to visualize modified training dataset having additional hidden_state column associated with it. [Against same six categories of emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "                   gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression, fit() (a Classifier linear_model from sklearn library)\n",
    "Creating a Classifier Model using LogisticRegression \n",
    "    1. The Classifier model is trained using data from modified emotions dataset that has additional column hidden_state in it.\n",
    "    2. Training is perfomed by supplying modified training dataset and calling fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We increase `max_iter` to guarantee convergence \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression, score() (a Classifier linear_model from sklearn library)\n",
    "Comparing output of Classifier model created by comparing its predictions on prediction set with actual values for validations set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising comparison of above output info by using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
